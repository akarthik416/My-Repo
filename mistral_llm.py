# -*- coding: utf-8 -*-
"""Mistral_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XjnRu6QEFGFRopQZa2xfawtlqQi94Uxe
"""

!pip install faiss-cpu
!pip install transformers
!pip install nltk
!pip install huggingface_hub

import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch
import nltk
from nltk.tokenize import sent_tokenize
import faiss
import numpy as np

# Load the Excel file
file_path = 'your_excel_file.xlsx'
df = pd.read_excel(file_path)

# Trim the data into two columns
# Assuming 'Column1' and 'Column2' are the columns of interest
trimmed_df = df[['Column1', 'Column2']]

# Combine the columns into a single text column (if needed)
trimmed_df['CombinedText'] = trimmed_df['Column1'].astype(str) + ' ' + trimmed_df['Column2'].astype(str)

# Extract the combined text as a list
texts = trimmed_df['CombinedText'].tolist()

# **Replace 'your_huggingface_token' with your actual token**
YOUR_HF_TOKEN = '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'

# Load the tokenizer and model for Mistral LLM
tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1', use_auth_token=YOUR_HF_TOKEN)
model = AutoModel.from_pretrained('mistralai/Mistral-7B-v0.1', use_auth_token=YOUR_HF_TOKEN)

# Function to get embeddings
def get_embeddings(texts):
    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
    with torch.no_grad():
        model_output = model(**encoded_input)
    embeddings = model_output.last_hidden_state.mean(dim=1)
    return embeddings

# Generate embeddings
embeddings = get_embeddings(texts)

# Download NLTK data
nltk.download('punkt')

# Function to chunk text into sentences
def chunk_text(text):
    return sent_tokenize(text)

# Apply chunking
chunked_texts = [chunk_text(text) for text in texts]
flat_chunked_texts = [sentence for sublist in chunked_texts for sentence in sublist]

# Convert embeddings to numpy array
embeddings_np = np.array([embedding.numpy() for embedding in embeddings])

# Initialize FAISS index
dimension = embeddings_np.shape[1]
index = faiss.IndexFlatL2(dimension)

# Add embeddings to the index
index.add(embeddings_np)

# Save the index
faiss.write_index(index, 'vector_index.faiss')

# To search the index
def search_index(query_text, index, top_k=5):
    query_embedding = get_embeddings([query_text]).numpy()
    D, I = index.search(query_embedding, top_k)
    return I

# Example search
query_text = "Your query text here"
top_k_results = search_index(query_text, index)
print(top_k_results)